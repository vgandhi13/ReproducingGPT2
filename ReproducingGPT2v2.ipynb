{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMkwQOwVDuaDfr6eLpN9BQN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgandhi13/ReproducingGPT2/blob/main/ReproducingGPT2v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KocgIDT_X9KS",
        "outputId": "fda7282b-a10c-4920-f501-a2a2366ca28f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-30 14:26:33--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-08-30 14:26:33 (17.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "loaded 338025 tokens\n",
            "1 epoch = 20 batches\n",
            "step 0, loss: 10.853811264038086, dt: 22578.78, tok/sec: 725.64\n",
            "step 1, loss: 10.080900192260742, dt: 43.87, tok/sec: 373459.18\n",
            "step 2, loss: 9.784505844116211, dt: 43.03, tok/sec: 380782.72\n",
            "step 3, loss: 9.665037155151367, dt: 42.94, tok/sec: 381562.89\n",
            "step 4, loss: 9.50172233581543, dt: 43.13, tok/sec: 379846.10\n",
            "step 5, loss: 9.463296890258789, dt: 43.10, tok/sec: 380144.47\n",
            "step 6, loss: 9.376127243041992, dt: 43.27, tok/sec: 378680.21\n",
            "step 7, loss: 9.238658905029297, dt: 43.11, tok/sec: 380037.26\n",
            "step 8, loss: 9.033145904541016, dt: 43.31, tok/sec: 378304.97\n",
            "step 9, loss: 8.883602142333984, dt: 43.01, tok/sec: 380941.03\n",
            "step 10, loss: 8.744534492492676, dt: 43.18, tok/sec: 379414.07\n",
            "step 11, loss: 8.63525676727295, dt: 44.12, tok/sec: 371328.17\n",
            "step 12, loss: 8.48263931274414, dt: 43.29, tok/sec: 378473.74\n",
            "step 13, loss: 8.377104759216309, dt: 43.72, tok/sec: 374734.03\n",
            "step 14, loss: 8.293031692504883, dt: 43.38, tok/sec: 377681.23\n",
            "step 15, loss: 8.118698120117188, dt: 43.87, tok/sec: 373449.03\n",
            "step 16, loss: 8.026004791259766, dt: 43.43, tok/sec: 377235.47\n",
            "step 17, loss: 7.936135292053223, dt: 43.22, tok/sec: 379085.47\n",
            "step 18, loss: 7.846698760986328, dt: 43.27, tok/sec: 378688.56\n",
            "step 19, loss: 7.643878936767578, dt: 43.24, tok/sec: 378893.18\n",
            "step 20, loss: 7.515983581542969, dt: 43.21, tok/sec: 379150.31\n",
            "step 21, loss: 7.278585433959961, dt: 43.48, tok/sec: 376817.63\n",
            "step 22, loss: 7.263241767883301, dt: 43.21, tok/sec: 379200.52\n",
            "step 23, loss: 7.138964653015137, dt: 43.19, tok/sec: 379338.67\n",
            "step 24, loss: 7.023563385009766, dt: 43.26, tok/sec: 378696.91\n",
            "step 25, loss: 7.07904052734375, dt: 43.24, tok/sec: 378930.78\n",
            "step 26, loss: 7.075051784515381, dt: 43.32, tok/sec: 378246.67\n",
            "step 27, loss: 6.952203750610352, dt: 42.96, tok/sec: 381344.80\n",
            "step 28, loss: 6.801275730133057, dt: 43.06, tok/sec: 380458.06\n",
            "step 29, loss: 6.67695426940918, dt: 43.33, tok/sec: 378134.28\n",
            "step 30, loss: 6.634268760681152, dt: 43.17, tok/sec: 379529.32\n",
            "step 31, loss: 6.606493949890137, dt: 43.45, tok/sec: 377036.78\n",
            "step 32, loss: 6.537430763244629, dt: 43.47, tok/sec: 376902.36\n",
            "step 33, loss: 6.602004051208496, dt: 43.12, tok/sec: 379957.41\n",
            "step 34, loss: 6.637972831726074, dt: 43.18, tok/sec: 379447.59\n",
            "step 35, loss: 6.486910820007324, dt: 43.14, tok/sec: 379825.10\n",
            "step 36, loss: 6.453525543212891, dt: 43.31, tok/sec: 378286.23\n",
            "step 37, loss: 6.456076145172119, dt: 43.24, tok/sec: 378903.62\n",
            "step 38, loss: 6.416883945465088, dt: 43.32, tok/sec: 378234.18\n",
            "step 39, loss: 6.269723892211914, dt: 43.96, tok/sec: 372738.04\n",
            "step 40, loss: 6.39691162109375, dt: 43.23, tok/sec: 378980.94\n",
            "step 41, loss: 6.199847221374512, dt: 43.94, tok/sec: 372869.50\n",
            "step 42, loss: 6.295155048370361, dt: 43.42, tok/sec: 377328.68\n",
            "step 43, loss: 6.190361499786377, dt: 43.08, tok/sec: 380314.88\n",
            "step 44, loss: 6.133823871612549, dt: 43.08, tok/sec: 380289.63\n",
            "step 45, loss: 6.338883399963379, dt: 43.15, tok/sec: 379686.59\n",
            "step 46, loss: 6.439476490020752, dt: 43.22, tok/sec: 379121.02\n",
            "step 47, loss: 6.3328399658203125, dt: 43.24, tok/sec: 378930.78\n",
            "step 48, loss: 6.2263593673706055, dt: 43.04, tok/sec: 380677.25\n",
            "step 49, loss: 6.13516902923584, dt: 43.01, tok/sec: 380953.70\n",
            "tensor(6.1352, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
            "torch.Size([16, 1024, 50257])\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    super().__init__()\n",
        "    #key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    #output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    #regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    #not really a bias more of a mask, but following Openai/HF naming\n",
        "    self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # batch, size, sequence legnth\n",
        "    #calculate query , key, values for all heads in batch and move head forward to be the batch\n",
        "    #nh is 'number of heads', hs is 'head size', and C (number of channels) = ns * hs\n",
        "    #eg in GPT-2 (124M), n_head = 12, hs = 654, so nh*hs = 768 channels in the transformer\n",
        "    qkv = self.c_attn(x)\n",
        "    q,k,v = qkv.split(self.n_embd, dim=2)\n",
        "    k = k.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    q = q.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    v = v.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    #attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "\n",
        "\n",
        "    att = (q @ k.transpose(-2, -1)) * (-1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B,nh, T, T) X [B, nh, T, hs] -> (B,nh,T,hs)\n",
        "\n",
        "\n",
        "\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C) # reassemble all head outputs side by side\n",
        "    #output projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate='tanh')\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 #max sequence length\n",
        "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftoken|>\n",
        "  n_layer: int = 6 #number of layers\n",
        "  n_head: int = 6 #number of heads\n",
        "  n_embd: int = 384 #embedding dimension\n",
        "\n",
        "class GPT(nn.Module):#turns into pytorch module\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict( #we can query using name of param (identical to GPT2)\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), #all the different blocks one after the other, #we can query using number of layer (identical to GPT2)\n",
        "        ln_f = nn.LayerNorm(config.n_embd) #additional thing added to transformer arch by openai\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) #final linear layer\n",
        "\n",
        "    #weight sharing scheme\n",
        "    #why we do this ?\n",
        "    # 1. this weight is 768*50257 = 40 mil params, (30% of 124M). So makes training more efficient\n",
        "    # 2.  Input embeddings and output embeddings represent the same space: tokens ↔ embeddings.\n",
        "    # 3. Tying ensures consistency: the representation used to encode a word is also used when predicting it.\n",
        "    # 4. Empirically, this improves perplexity\n",
        "    self.transformer.wte.weight = self.lm_head.weight #(vocab_size, n_embd)\n",
        "\n",
        "    #init params\n",
        "    #iteralte all modules here\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  #Prevent vanishing/exploding activations\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      #per-layer scaling trick applied to some linear layers (specifically the projection layers inside attention and MLP blocks)\n",
        "      #It rescales the initialization standard deviation to account for residual connections.\n",
        "      #This way, variance is preserved across depth, so activations don’t explode/vanish.\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * module.NANOGPT_SCALE_INIT) ** -0.5  #for variance handling\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # idx is of shape (B,T) , token indices, batch dim of B, time dim of upto t, t\n",
        "    B, T = idx.size()\n",
        "    assert T<= self.config.block_size, f\"Cannot forward sequence of length {T},block size is\"\n",
        "    #forward the token and position embeddings\n",
        "    pos = torch.arange(0, T, dtype = torch.long, device = idx.device) #shape(T)\n",
        "    pos_emb = self.transformer.wpe(pos) #POSITION EMBEDDINGS of shape (T,n_embd)\n",
        "    tok_emb = self.transformer.wte(idx) #token embeddings of shape (B,T,n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "    #forward the blocks of the transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.transformer.ln_f(x)  #(B,T,n_embd)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) - essentially give for each batch, gives t+1th probable element\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      # here cross entropy cannot take 3 dim vector, so it is flattening it by first making logits of shape (B*T, vocab_size), and targets to shape (B*T)\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "    '''Loads pretrained GPT2 model weights from huggingface'''\n",
        "    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "    #n_layer, n_head, and n_embd are determined from model_type\n",
        "    config_args = {\n",
        "        'gpt2': dict(n_layer=12, n_head=12, n_embd=768), #124M Params\n",
        "        'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), #350M params\n",
        "        'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280), #774M params\n",
        "        'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600), #1558M params\n",
        "    }[model_type]\n",
        "    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "    # create a from scratch initialized miniGPT model\n",
        "    config = GPTConfig(**config_args)\n",
        "    model = GPT(config)\n",
        "    sd = model.state_dict()\n",
        "    sd_keys = sd.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] #discard this mask/buffer\n",
        "\n",
        "    #init a huggingface trasformer model\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    sd_hf = model_hf.state_dict()\n",
        "\n",
        "    sd_keys_hf = sd_hf.keys()\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] #discard this mask/buff\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "    assert len(sd_keys) == len(sd_keys_hf), f\"mismatched keys: {len(sd_keys)} != {len(sd_keys_hf)}\"\n",
        "    for k in sd_keys:\n",
        "      if any(k.endswith(w) for w in transposed):\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k].t())\n",
        "      else:\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k])\n",
        "    return model\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#DATA LOADER\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    #load tiny shakespeare dataset\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "\n",
        "    #state\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "\n",
        "    # Now, Process token sequences and feed them into transformer. Rearrange tokens into idx variable feeding into transformer.\n",
        "    # We dont want single very long one dimensional sequence, we want a batch where each sequence is upto T tokens (T cannot be larger than maximum sequence length).\n",
        "    # We have B indpendent examples of T sequences. So we, need to create a (B,T) tensor which we can feed to the forward out of this 1 dimensional sequences\n",
        "    buf = self.tokens[self.current_position : self.current_position+B*T+1] # B*T + 1 because we need the next token of the last (B*Tth) token as well for training\n",
        "    x = buf[:-1].view(B,T) #we exclude last one because it is the extra target token for (B*Tth) token\n",
        "    y = buf[1:].view(B,T) # we exclude first one because it is not a target for any token\n",
        "\n",
        "    #advance the position in the tensor\n",
        "    self.current_position += B*T\n",
        "    #if loading the next batch would be out of bounds, reset\n",
        "    if self.current_position + (B*T+1) > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#TRAINING\n",
        "\n",
        "\n",
        "# setting up gpu use\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "\n",
        "#for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=16, T=1024)\n",
        "torch.set_float32_matmul_precision('high') #for faster training. Every where there is a multiplcation in our linear layers, pytorch will now run this mult on cores, utilizung tf32 precision\n",
        "\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "# model.eval()\n",
        "# model.to(device)\n",
        "# print('didnt crash yay!!')\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "#compile for neural nets, like gcc in c. makes stuff faster. reduces python overhead and GPU read/writes\n",
        "#unlike python interpreter that looks at code sequentially, this looks at all the operations ahead and time and then optimizes things based on that.\n",
        "model = torch.compile(model)\n",
        "\n",
        "import time\n",
        "#optimize!!\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16): # TF32 for FP32 matmuls outside autocast. BF16 for eligible ops inside autocast, FP32 for critical ops (loss, gradients, optimizer state)\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize() #cpu might already reach here, but this will wait for gpus to finish the work they were assigned. is only needed for benchmarking. For real training you can drop it (but keep it if you want accurate dt timings).\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000 #time difference in miliseconds\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1-t0)\n",
        "  print(f'step {i}, loss: {loss.item()}, dt: {dt:.2f}, tok/sec: {tokens_per_sec:.2f}')\n",
        "\n",
        "\n",
        "print(loss)\n",
        "print(logits.shape)\n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "1. In pytorch you can directly do model.to(device), but for tensors, you need to save them in variable x = x.to(device)\n",
        "2. All tensors in pytorch by default are float32 which 19.5 teraflops on A100. If numbers have fewer bits of representation, it is easier to move them around. We have a finite capacity of bits our GPU can store but in addition to that there is a limitation to the speed with which we can access. Many of deep learning workload memory bounds, so most of tensor cores are idle because they are waiting around for data. So even if we get 60% utilization of hardware, we are doing well\n",
        "3. Most of the computation happens in linear layers. Entire transformer is bunch of matrix multiplicaton. Biggest one is the classifier layer at the top that goes from 768 to 50257. Matrix mult becomes faster by lowering precision\n",
        "4. TF32 is 13 bits lesser in mantissa than FP32. Inputs and outputs are fp32, but internally it switches to tf32 and this increases speed by 8 times, and we cannot tell much difference in the results.\n",
        "5. Numbers like 16 and 32 are good for Batches and Time. But something like 17 is bad.\n",
        "6. Lowering precision in model training typically decreases training time because lower-precision data (like FP16 compared to FP32) allows hardware to perform computations faster, requires less memory bandwidth, and enables higher parallelism—so more operations happen per clock cycle. However, we expect slight less accurate results but empirically this is a worth it tradeoff. Because you can train longer to make up for the precision decrease\n",
        "7. HBM is connected with GPU. GPU is where most calc happens, but it also has some memory. Most of memory is in high bandwidth memory(HBM). These are two separate chips. HBM is off chip. On GPU, there are large number of streaming processor all of which are SM, and this is where lot of the calculations happen. Single SM has 4 quadrants, each has a tensor core, and different subunits where calcs happen. On GPU chips there is L2 cache, but then on the SM in GPU, there is l1 cache,and registers. The way the memory is stored on GPU is quite different from HBM. So, there is meomery inside GUP but it is not a lot of memory.\n",
        "8. Now, even if main memory of computer is very large, it is very inefficient as GPU would have to go through CPU to reach disk, and this is very time intensive. Then GPUs have HBM which are large in memory, but are also expensive to access. Then, on the GPU chips itself everything is very fast, but there is very few memory on it(in MBs as opposed to gbS), but it is lightning fast. So basically whenver, we have these kernels, we take these inputs which live on HBM, we start streaming data to gpu chip, we do computations, and then send it back to HBM. If we dont use torch.compile, we we are doing this HBM->GPU->HBM transfers many more times. But when we use it, since it already know stuff ahead in time, it optimizes data transfer and does all the computations together. So operation fusion, allows to keep chunk of data on chip, do lots of computation, and then do a single transfer back to HBM.\n",
        "9. Flash attention: Fusing matmul, dropout, softmax, mask, and matmul to a fused kernel of flash attention. It is a kernel fusing algorithm torch.compile cannot find, and the reason is algorithmic rewrite of how attention is implemented. Flashattention does more flops(arithmetic ops) than regular attention. But it is significantly faster(7.6x) because very mindful of the memory heirarchy described above. Very mindful about what is in HBM, what is in shared memory. Very careful of how it orchastrates the computation, such that we have a fewer reads and writes with HBM. So even though we are doing more flops, the expensive part is the load and store. The NXN matrix (att in our code), never gets materialized at any point in HBM, and never gets read or written to HBM. For each head,"
      ],
      "metadata": {
        "id": "iiLlZe0lXFj_"
      }
    }
  ]
}