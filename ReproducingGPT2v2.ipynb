{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOblu9IxHVca+OdqUiGgFiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgandhi13/ReproducingGPT2/blob/main/ReproducingGPT2v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KocgIDT_X9KS"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    super().__init__()\n",
        "    #key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    #output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    #regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    #not really a bias more of a mask, but following Openai/HF naming\n",
        "    self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # batch, size, sequence legnth\n",
        "    #calculate query , key, values for all heads in batch and move head forward to be the batch\n",
        "    #nh is 'number of heads', hs is 'head size', and C (number of channels) = ns * hs\n",
        "    #eg in GPT-2 (124M), n_head = 12, hs = 654, so nh*hs = 768 channels in the transformer\n",
        "    qkv = self.c_attn(x)\n",
        "    q,k,v = qkv.split(self.n_embd, dim=2)\n",
        "    k = k.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    q = q.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    v = v.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    #attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "    att = (q @ k.transpose(-2, -1)) * (-1.0 / math.sqrt(k.size(-1)))\n",
        "    att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "    att = F.softmax(att, dim=-1)\n",
        "    y = att @ v # (B,nh, T, T) X [B, nh, T, hs] -> (B,nh,T,hs)\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C) # reassemble all head outputs side by side\n",
        "    #output projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate='tanh')\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 #max sequence length\n",
        "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftoken|>\n",
        "  n_layer: int = 6 #number of layers\n",
        "  n_head: int = 6 #number of heads\n",
        "  n_embd: int = 384 #embedding dimension\n",
        "\n",
        "class GPT(nn.Module):#turns into pytorch module\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict( #we can query using name of param (identical to GPT2)\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), #all the different blocks one after the other, #we can query using number of layer (identical to GPT2)\n",
        "        ln_f = nn.LayerNorm(config.n_embd) #additional thing added to transformer arch by openai\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) #final linear layer\n",
        "\n",
        "    #weight sharing scheme\n",
        "    #why we do this ?\n",
        "    # 1. this weight is 768*50257 = 40 mil params, (30% of 124M). So makes training more efficient\n",
        "    # 2.  Input embeddings and output embeddings represent the same space: tokens ↔ embeddings.\n",
        "    # 3. Tying ensures consistency: the representation used to encode a word is also used when predicting it.\n",
        "    # 4. Empirically, this improves perplexity\n",
        "    self.transformer.wte.weight = self.lm_head.weight #(vocab_size, n_embd)\n",
        "\n",
        "    #init params\n",
        "    #iteralte all modules here\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  #Prevent vanishing/exploding activations\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      #per-layer scaling trick applied to some linear layers (specifically the projection layers inside attention and MLP blocks)\n",
        "      #It rescales the initialization standard deviation to account for residual connections.\n",
        "      #This way, variance is preserved across depth, so activations don’t explode/vanish.\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * module.NANOGPT_SCALE_INIT) ** -0.5  #for variance handling\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # idx is of shape (B,T) , token indices, batch dim of B, time dim of upto t, t\n",
        "    B, T = idx.size()\n",
        "    assert T<= self.config.block_size, f\"Cannot forward sequence of length {T},block size is\"\n",
        "    #forward the token and position embeddings\n",
        "    pos = torch.arange(0, T, dtype = torch.long, device = idx.device) #shape(T)\n",
        "    pos_emb = self.transformer.wpe(pos) #POSITION EMBEDDINGS of shape (T,n_embd)\n",
        "    tok_emb = self.transformer.wte(idx) #token embeddings of shape (B,T,n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "    #forward the blocks of the transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.transformer.ln_f(x)  #(B,T,n_embd)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) - essentially give for each batch, gives t+1th probable element\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      # here cross entropy cannot take 3 dim vector, so it is flattening it by first making logits of shape (B*T, vocab_size), and targets to shape (B*T)\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "    '''Loads pretrained GPT2 model weights from huggingface'''\n",
        "    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "    #n_layer, n_head, and n_embd are determined from model_type\n",
        "    config_args = {\n",
        "        'gpt2': dict(n_layer=12, n_head=12, n_embd=768), #124M Params\n",
        "        'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), #350M params\n",
        "        'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280), #774M params\n",
        "        'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600), #1558M params\n",
        "    }[model_type]\n",
        "    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "    # create a from scratch initialized miniGPT model\n",
        "    config = GPTConfig(**config_args)\n",
        "    model = GPT(config)\n",
        "    sd = model.state_dict()\n",
        "    sd_keys = sd.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] #discard this mask/buffer\n",
        "\n",
        "    #init a huggingface trasformer model\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    sd_hf = model_hf.state_dict()\n",
        "\n",
        "    sd_keys_hf = sd_hf.keys()\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] #discard this mask/buff\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "    assert len(sd_keys) == len(sd_keys_hf), f\"mismatched keys: {len(sd_keys)} != {len(sd_keys_hf)}\"\n",
        "    for k in sd_keys:\n",
        "      if any(k.endswith(w) for w in transposed):\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k].t())\n",
        "      else:\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k])\n",
        "    return model\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    #load tiny shakespeare dataset\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "\n",
        "    #state\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "\n",
        "    # Now, Process token sequences and feed them into transformer. Rearrange tokens into idx variable feeding into transformer.\n",
        "    # We dont want single very long one dimensional sequence, we want a batch where each sequence is upto T tokens (T cannot be larger than maximum sequence length).\n",
        "    # We have B indpendent examples of T sequences. So we, need to create a (B,T) tensor which we can feed to the forward out of this 1 dimensional sequences\n",
        "    buf = self.tokens[self.current_position : self.current_position+B*T+1] # B*T + 1 because we need the next token of the last (B*Tth) token as well for training\n",
        "    x = buf[:-1].view(B,T) #we exclude last one because it is the extra target token for (B*Tth) token\n",
        "    y = buf[1:].view(B,T) # we exclude first one because it is not a target for any token\n",
        "\n",
        "    #advance the position in the tensor\n",
        "    self.current_position += B*T\n",
        "    #if loading the next batch would be out of bounds, reset\n",
        "    if self.current_position + (B*T+1) > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# setting up gpu use\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "\n",
        "#for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader = DataLoaderLite(B=16, T=1024)\n",
        "torch.set_float32_matmul_precision('high') #for faster training. Every where there is a multiplcation in our linear layers, pytorch will now run this mult on cores, utilizung tf32 precision\n",
        "\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "# model.eval()\n",
        "# model.to(device)\n",
        "# print('didnt crash yay!!')\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "import time\n",
        "#optimize!!\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
        "for i in range(50):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize() #cpu might already reach here, but this will wait for gpus to finish the work they were assigned\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000 #time difference in miliseconds\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1-t0)\n",
        "  print(f'step {i}, loss: {loss.item()}, dt: {dt:.2f}, tok/sec: {tokens_per_sec:.2f}')\n",
        "\n",
        "\n",
        "print(loss)\n",
        "print(logits.shape)\n",
        "#\n"
      ]
    }
  ]
}