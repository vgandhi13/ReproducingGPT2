{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMHvB/fQpM8wuX9woPWcqdO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgandhi13/ReproducingGPT2/blob/main/ReproducingGPT2v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KocgIDT_X9KS",
        "outputId": "9bd72cb4-a4af-4599-9f2b-a7b684520e11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-31 18:10:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "\rinput.txt.2           0%[                    ]       0  --.-KB/s               \rinput.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-08-31 18:10:28 (136 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n",
            "loaded 338025 tokens\n",
            "1 epoch = 20 batches\n",
            "step 0, loss: 10.963605880737305, dt: 23370.15, tok/sec: 701.07\n",
            "step 1, loss: 9.480354309082031, dt: 92.52, tok/sec: 177076.91\n",
            "step 2, loss: 9.205926895141602, dt: 92.97, tok/sec: 176236.77\n",
            "step 3, loss: 9.006126403808594, dt: 92.86, tok/sec: 176440.40\n",
            "step 4, loss: 8.790132522583008, dt: 93.04, tok/sec: 176089.55\n",
            "step 5, loss: 8.672025680541992, dt: 93.35, tok/sec: 175516.57\n",
            "step 6, loss: 8.43651008605957, dt: 93.12, tok/sec: 175949.34\n",
            "step 7, loss: 8.162247657775879, dt: 93.51, tok/sec: 175218.07\n",
            "step 8, loss: 7.893044471740723, dt: 93.05, tok/sec: 176069.70\n",
            "step 9, loss: 7.716394424438477, dt: 93.75, tok/sec: 174771.56\n",
            "step 10, loss: 7.534371376037598, dt: 93.59, tok/sec: 175054.26\n",
            "step 11, loss: 7.401754856109619, dt: 93.46, tok/sec: 175312.39\n",
            "step 12, loss: 7.195182800292969, dt: 93.21, tok/sec: 175768.42\n",
            "step 13, loss: 7.129495620727539, dt: 93.48, tok/sec: 175268.12\n",
            "step 14, loss: 7.096128463745117, dt: 93.08, tok/sec: 176019.19\n",
            "step 15, loss: 6.9382171630859375, dt: 93.40, tok/sec: 175420.24\n",
            "step 16, loss: 6.901089668273926, dt: 93.90, tok/sec: 174490.65\n",
            "step 17, loss: 6.841031074523926, dt: 93.09, tok/sec: 176010.18\n",
            "step 18, loss: 6.815688610076904, dt: 93.62, tok/sec: 175001.66\n",
            "step 19, loss: 6.670440673828125, dt: 93.69, tok/sec: 174874.74\n",
            "step 20, loss: 6.570898056030273, dt: 92.96, tok/sec: 176244.91\n",
            "step 21, loss: 6.370438098907471, dt: 93.61, tok/sec: 175022.16\n",
            "step 22, loss: 6.42430305480957, dt: 93.71, tok/sec: 174841.37\n",
            "step 23, loss: 6.397794246673584, dt: 93.50, tok/sec: 175221.65\n",
            "step 24, loss: 6.3406782150268555, dt: 94.16, tok/sec: 173994.06\n",
            "step 25, loss: 6.554983139038086, dt: 93.66, tok/sec: 174924.59\n",
            "step 26, loss: 6.625049591064453, dt: 93.58, tok/sec: 175088.61\n",
            "step 27, loss: 6.521013259887695, dt: 93.57, tok/sec: 175097.53\n",
            "step 28, loss: 6.464198112487793, dt: 93.41, tok/sec: 175403.23\n",
            "step 29, loss: 6.362513542175293, dt: 93.29, tok/sec: 175630.06\n",
            "step 30, loss: 6.380640029907227, dt: 93.70, tok/sec: 174850.27\n",
            "step 31, loss: 6.417862892150879, dt: 93.47, tok/sec: 175288.69\n",
            "step 32, loss: 6.358239650726318, dt: 93.61, tok/sec: 175019.93\n",
            "step 33, loss: 6.397089004516602, dt: 93.55, tok/sec: 175140.82\n",
            "step 34, loss: 6.4804792404174805, dt: 94.30, tok/sec: 173737.16\n",
            "step 35, loss: 6.351605415344238, dt: 94.02, tok/sec: 174252.60\n",
            "step 36, loss: 6.336646556854248, dt: 93.16, tok/sec: 175868.28\n",
            "step 37, loss: 6.347595691680908, dt: 93.94, tok/sec: 174409.16\n",
            "step 38, loss: 6.321160316467285, dt: 93.61, tok/sec: 175030.63\n",
            "step 39, loss: 6.16425895690918, dt: 93.42, tok/sec: 175375.03\n",
            "step 40, loss: 6.3494367599487305, dt: 94.01, tok/sec: 174282.66\n",
            "step 41, loss: 6.110628128051758, dt: 93.71, tok/sec: 174845.82\n",
            "step 42, loss: 6.235225677490234, dt: 93.48, tok/sec: 175271.25\n",
            "step 43, loss: 6.139448165893555, dt: 93.82, tok/sec: 174637.87\n",
            "step 44, loss: 6.08908748626709, dt: 94.03, tok/sec: 174243.33\n",
            "step 45, loss: 6.29539680480957, dt: 93.59, tok/sec: 175068.09\n",
            "step 46, loss: 6.412628650665283, dt: 94.63, tok/sec: 173133.12\n",
            "step 47, loss: 6.288034439086914, dt: 93.46, tok/sec: 175308.37\n",
            "step 48, loss: 6.22643518447876, dt: 93.83, tok/sec: 174621.89\n",
            "step 49, loss: 6.135463714599609, dt: 94.44, tok/sec: 173493.29\n",
            "tensor(6.1355, device='cuda:0', grad_fn=<CompiledFunctionBackward>)\n",
            "torch.Size([16, 1024, 50304])\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    super().__init__()\n",
        "    #key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    #output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    #regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    #not really a bias more of a mask, but following Openai/HF naming\n",
        "    self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # batch, size, sequence legnth\n",
        "    #calculate query , key, values for all heads in batch and move head forward to be the batch\n",
        "    #nh is 'number of heads', hs is 'head size', and C (number of channels) = ns * hs\n",
        "    #eg in GPT-2 (124M), n_head = 12, hs = 654, so nh*hs = 768 channels in the transformer\n",
        "    qkv = self.c_attn(x) #(B,T,3*C)\n",
        "    q,k,v = qkv.split(self.n_embd, dim=2) #(B,T,C), (B,T,C), (B,T,C)\n",
        "    #nh is number of heads, hs is head size\n",
        "    k = k.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    q = q.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    v = v.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    #attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "\n",
        "    #REGULAR ATTENTION APPROACH\n",
        "    # att = (q @ k.transpose(-2, -1)) * (-1.0 / math.sqrt(k.size(-1)))\n",
        "    # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B,nh, T, T) X [B, nh, T, hs] -> (B,nh,T,hs)\n",
        "\n",
        "    #FLASH ATTENTION APPROACH\n",
        "    y = F.scaled_dot_product_attention(q,k,v, is_causal=True)\n",
        "\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C) # reassemble all head outputs side by side\n",
        "    #output projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate='tanh')\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 #max sequence length\n",
        "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftoken|>\n",
        "  n_layer: int = 12 #number of layers\n",
        "  n_head: int = 12 #number of heads\n",
        "  n_embd: int = 768 #embedding dimension\n",
        "\n",
        "class GPT(nn.Module):#turns into pytorch module\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict( #we can query using name of param (identical to GPT2)\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), #all the different blocks one after the other, #we can query using number of layer (identical to GPT2)\n",
        "        ln_f = nn.LayerNorm(config.n_embd) #additional thing added to transformer arch by openai\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) #final linear layer\n",
        "\n",
        "    #weight sharing scheme\n",
        "    #why we do this ?\n",
        "    # 1. this weight is 768*50257 = 40 mil params, (30% of 124M). So makes training more efficient\n",
        "    # 2.  Input embeddings and output embeddings represent the same space: tokens ↔ embeddings.\n",
        "    # 3. Tying ensures consistency: the representation used to encode a word is also used when predicting it.\n",
        "    # 4. Empirically, this improves perplexity\n",
        "    self.transformer.wte.weight = self.lm_head.weight #(vocab_size, n_embd)\n",
        "\n",
        "    #init params\n",
        "    #iteralte all modules here\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  #Prevent vanishing/exploding activations\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      #per-layer scaling trick applied to some linear layers (specifically the projection layers inside attention and MLP blocks)\n",
        "      #It rescales the initialization standard deviation to account for residual connections.\n",
        "      #This way, variance is preserved across depth, so activations don’t explode/vanish.\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * module.NANOGPT_SCALE_INIT) ** -0.5  #for variance handling\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # idx is of shape (B,T) , token indices, batch dim of B, time dim of upto t, t\n",
        "    B, T = idx.size()\n",
        "    assert T<= self.config.block_size, f\"Cannot forward sequence of length {T},block size is\"\n",
        "    #forward the token and position embeddings\n",
        "    pos = torch.arange(0, T, dtype = torch.long, device = idx.device) #shape(T)\n",
        "    pos_emb = self.transformer.wpe(pos) #POSITION EMBEDDINGS of shape (T,n_embd)\n",
        "    tok_emb = self.transformer.wte(idx) #token embeddings of shape (B,T,n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "    #forward the blocks of the transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.transformer.ln_f(x)  #(B,T,n_embd)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) - essentially give for each batch, gives t+1th probable element\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      # here cross entropy cannot take 3 dim vector, so it is flattening it by first making logits of shape (B*T, vocab_size), and targets to shape (B*T)\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "    '''Loads pretrained GPT2 model weights from huggingface'''\n",
        "    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "    #n_layer, n_head, and n_embd are determined from model_type\n",
        "    config_args = {\n",
        "        'gpt2': dict(n_layer=12, n_head=12, n_embd=768), #124M Params\n",
        "        'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), #350M params\n",
        "        'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280), #774M params\n",
        "        'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600), #1558M params\n",
        "    }[model_type]\n",
        "    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "    # create a from scratch initialized miniGPT model\n",
        "    config = GPTConfig(**config_args)\n",
        "    model = GPT(config)\n",
        "    sd = model.state_dict()\n",
        "    sd_keys = sd.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] #discard this mask/buffer\n",
        "\n",
        "    #init a huggingface trasformer model\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    sd_hf = model_hf.state_dict()\n",
        "\n",
        "    sd_keys_hf = sd_hf.keys()\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] #discard this mask/buff\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "    assert len(sd_keys) == len(sd_keys_hf), f\"mismatched keys: {len(sd_keys)} != {len(sd_keys_hf)}\"\n",
        "    for k in sd_keys:\n",
        "      if any(k.endswith(w) for w in transposed):\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k].t())\n",
        "      else:\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k])\n",
        "    return model\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#DATA LOADER\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    #load tiny shakespeare dataset\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "\n",
        "    #state\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "\n",
        "    # Now, Process token sequences and feed them into transformer. Rearrange tokens into idx variable feeding into transformer.\n",
        "    # We dont want single very long one dimensional sequence, we want a batch where each sequence is upto T tokens (T cannot be larger than maximum sequence length).\n",
        "    # We have B indpendent examples of T sequences. So we, need to create a (B,T) tensor which we can feed to the forward out of this 1 dimensional sequences\n",
        "    buf = self.tokens[self.current_position : self.current_position+B*T+1] # B*T + 1 because we need the next token of the last (B*Tth) token as well for training\n",
        "    x = buf[:-1].view(B,T) #we exclude last one because it is the extra target token for (B*Tth) token\n",
        "    y = buf[1:].view(B,T) # we exclude first one because it is not a target for any token\n",
        "\n",
        "    #advance the position in the tensor\n",
        "    self.current_position += B*T\n",
        "    #if loading the next batch would be out of bounds, reset\n",
        "    if self.current_position + (B*T+1) > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#TRAINING\n",
        "\n",
        "\n",
        "# setting up gpu use\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "\n",
        "#for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(1337)\n",
        "\n",
        "\n",
        "train_loader = DataLoaderLite(B=16, T=1024)\n",
        "torch.set_float32_matmul_precision('high') #for faster training. Every where there is a multiplcation in our linear layers, pytorch will now run this mult on cores, utilizung tf32 precision\n",
        "\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "# model.eval()\n",
        "# model.to(device)\n",
        "# print('didnt crash yay!!')\n",
        "model = GPT(GPTConfig(vocab_size=50304)) #padding of tokens to improve speed. check notes(11)\n",
        "model.to(device)\n",
        "\n",
        "#compile for neural nets, like gcc in c. makes stuff faster. reduces python overhead and GPU read/writes\n",
        "#unlike python interpreter that looks at code sequentially, this looks at all the operations ahead and time and then optimizes things based on that.\n",
        "model = torch.compile(model)\n",
        "\n",
        "##Learning Rate Scheduler\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  #1. linear warmup for warmup_iters steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps #LR ramping up linearly\n",
        "  #2. if it > lr_decay_iters, return min learning rate. Learning rate stays at min_lr (flat) after training schedule finishes.\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "  #3. in between, use cosine decay down to min learning rate\n",
        "  #LR decays cosine-shaped from max_lr down to min_lr\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) #coeff starts at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "#optimize!!\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  x, y = train_loader.next_batch()\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  with torch.autocast(device_type=device, dtype=torch.bfloat16): # TF32 for FP32 matmuls outside autocast. BF16 for eligible ops inside autocast, FP32 for critical ops (loss, gradients, optimizer state)\n",
        "    logits, loss = model(x, y)\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #calculate global norm of params, every single grad on all params, we square it, add it all up, and take a big square root and this is the norm of param vector. check note(13)\n",
        "  #determine and set the learning rate for this iteration\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups: #there is a notion of diff param groups that can exist in optimizer. in our case there is just 1, but we have to do this\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize() #cpu might already reach here, but this will wait for gpus to finish the work they were assigned. is only needed for benchmarking. For real training you can drop it (but keep it if you want accurate dt timings).\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000 #time difference in miliseconds\n",
        "  tokens_per_sec = (train_loader.B * train_loader.T) / (t1-t0)\n",
        "  print(f'step {i}, loss: {loss.item()}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt:.2f}, tok/sec: {tokens_per_sec:.2f}')\n",
        "\n",
        "\n",
        "print(loss)\n",
        "print(logits.shape)\n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "1. In pytorch you can directly do model.to(device), but for tensors, you need to save them in variable x = x.to(device)\n",
        "2. All tensors in pytorch by default are float32 which 19.5 teraflops on A100. If numbers have fewer bits of representation, it is easier to move them around. We have a finite capacity of bits our GPU can store but in addition to that there is a limitation to the speed with which we can access. Many of deep learning workload memory bounds, so most of tensor cores are idle because they are waiting around for data. So even if we get 60% utilization of hardware, we are doing well\n",
        "3. Most of the computation happens in linear layers. Entire transformer is bunch of matrix multiplicaton. Biggest one is the classifier layer at the top that goes from 768 to 50257. Matrix mult becomes faster by lowering precision\n",
        "4. TF32 is 13 bits lesser in mantissa than FP32. Inputs and outputs are fp32, but internally it switches to tf32 and this increases speed by 8 times, and we cannot tell much difference in the results.\n",
        "5. Numbers like 16 and 32 are good for Batches and Time. But something like 17 is bad.\n",
        "6. Lowering precision in model training typically decreases training time because lower-precision data (like FP16 compared to FP32) allows hardware to perform computations faster, requires less memory bandwidth, and enables higher parallelism—so more operations happen per clock cycle. However, we expect slight less accurate results but empirically this is a worth it tradeoff. Because you can train longer to make up for the precision decrease\n",
        "7. HBM is connected with GPU. GPU is where most calc happens, but it also has some memory. Most of memory is in high bandwidth memory(HBM). These are two separate chips. HBM is off chip. On GPU, there are large number of streaming processor all of which are SM, and this is where lot of the calculations happen. Single SM has 4 quadrants, each has a tensor core, and different subunits where calcs happen. On GPU chips there is L2 cache, but then on the SM in GPU, there is l1 cache,and registers. The way the memory is stored on GPU is quite different from HBM. So, there is meomery inside GUP but it is not a lot of memory.\n",
        "8. Now, even if main memory of computer is very large, it is very inefficient as GPU would have to go through CPU to reach disk, and this is very time intensive. Then GPUs have HBM which are large in memory, but are also expensive to access. Then, on the GPU chips itself everything is very fast, but there is very few memory on it(in MBs as opposed to gbS), but it is lightning fast. So basically whenver, we have these kernels, we take these inputs which live on HBM, we start streaming data to gpu chip, we do computations, and then send it back to HBM. If we dont use torch.compile, we we are doing this HBM->GPU->HBM transfers many more times. But when we use it, since it already know stuff ahead in time, it optimizes data transfer and does all the computations together. So operation fusion, allows to keep chunk of data on chip, do lots of computation, and then do a single transfer back to HBM.\n",
        "9. Flash attention: Fusing matmul, dropout, softmax, mask, and matmul to a fused kernel of flash attention. It is a kernel fusing algorithm torch.compile cannot find, and the reason is algorithmic rewrite of how attention is implemented. Flashattention does more flops(arithmetic ops) than regular attention. But it is significantly faster(7.6x) because very mindful of the memory heirarchy described above. Very mindful about what is in HBM, what is in shared memory. Very careful of how it orchastrates the computation, such that we have a fewer reads and writes with HBM. So even though we are doing more flops, the expensive part is the load and store. The NXN matrix (att in our code), never gets materialized at any point in HBM, and never gets read or written to HBM. For each head,\n",
        "10. It is good to deal with powers of two because that is how cuda works\n",
        "11. When we pass in custom vocab_size=50304, which is > 50257, (to make a an ugly number into a nicer number power of 2 to increase speed), wte becomes larger, but these newer tokens rows/vectors are never used because GPT2 only has 50257 tokens. We will never index into these rows, so wasting a little bit of space. Now, it is share with classifier at the end,  so we are predicting probabilities for tokens that will never be present in the training set and so therefore the network has to learn that these probabilities have to be driven to 0. And so the logits that the network produces have to drive those dimensions of the output to negative infinity. But this is no different to tokens not present in our dataset, ie Shakespeare dataset only uses 1000 of the 50000+ tokens. So functionally nothing breaks, we just use extra memory. It is running faster because many kernels use block tiles which are powers of 2, so calcs done in chunks of 64. When desired calc does not neatly fit into those block tiles, there are all kind of boundary kernels that can kick in to do the last part. In a lot of kernels, they will truncate up your input and will do the nice part first and then they have a whole second phase where they come back to anything that remains and process it, but this could be very inefficient. So instead pad the input, and make it fit nicely.\n",
        "12. According to Andrej, gpt 2 has open weights, but paper does not have much detail for training, for gpt 3, paper has a lot of info, but weights not released. Roughly speaking they are very similar architecturally  , apart from some hyperparameters,context length size, training time, more data\n",
        "13. We make sure length is not more than 1.0. People like to use this gradient norm clipping, because sometimes you can get unlucky with optimization because of bad data batch or something like that. If you get very unlucky in batch you might get really high loss and really high loss could lead to really high gradient and this could basically shock your model and the optimization . So this prevents model from getting too big of shocks in terms of gradient magnitudes. Bit of a hacky solution, patch on top of bigger issues. You can visualize this norm, and if norm of gradient is horizontal, it is well behaved and fine, if it is climbing, things are bad, sometimes you can get spike which is also bad due to training instability. Norm very high in begining as learning a lot of new stuff.\n",
        "14. For context, I trained the model on A100 GPU\n",
        "15. Cosine learning rate that we used here has been popularize by gpt 2 and 3 papers, but there are other learning rate schedules and this is an active area of research."
      ],
      "metadata": {
        "id": "iiLlZe0lXFj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30"
      ],
      "metadata": {
        "id": "5CxvvkgNwDbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78tIQpndwDYz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}