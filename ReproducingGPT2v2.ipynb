{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNE+ootVNuhu03g1vS16wnF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgandhi13/ReproducingGPT2/blob/main/ReproducingGPT2v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KocgIDT_X9KS",
        "outputId": "92b40508-4caf-4c75-e581-d2a89594dfb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "total desired batch size: 524288\n",
            "=> calculated gradient accumulation steps: 32\n",
            "--2025-09-03 13:17:05--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-09-03 13:17:05 (130 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "loaded 338025 tokens\n",
            "1 epoch = 20 batches\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW:  True\n",
            "step 0, loss: 10.964640617370605, norm: 12.4695, lr: 6.0000e-05, dt: 27399.71, tok/sec: 19134.80\n",
            "step 1, loss: 9.789239883422852, norm: 5.7654, lr: 1.2000e-04, dt: 2755.28, tok/sec: 190285.13\n",
            "step 2, loss: 9.352588653564453, norm: 5.3757, lr: 1.8000e-04, dt: 2755.31, tok/sec: 190282.78\n",
            "step 3, loss: 9.647992134094238, norm: 5.5522, lr: 2.4000e-04, dt: 2758.58, tok/sec: 190056.96\n",
            "step 4, loss: 9.036356925964355, norm: 3.8419, lr: 3.0000e-04, dt: 2760.67, tok/sec: 189913.54\n",
            "step 5, loss: 8.618415832519531, norm: 2.9223, lr: 3.6000e-04, dt: 2763.90, tok/sec: 189691.10\n",
            "step 6, loss: 8.369525909423828, norm: 3.0591, lr: 4.2000e-04, dt: 2763.97, tok/sec: 189686.60\n",
            "step 7, loss: 8.067736625671387, norm: 2.2751, lr: 4.8000e-04, dt: 2761.46, tok/sec: 189859.02\n",
            "step 8, loss: 7.720681667327881, norm: 2.1053, lr: 5.4000e-04, dt: 2766.79, tok/sec: 189492.94\n",
            "step 9, loss: 7.391160011291504, norm: 2.3132, lr: 6.0000e-04, dt: 2760.68, tok/sec: 189912.42\n",
            "step 10, loss: 7.081424713134766, norm: 2.0389, lr: 6.0000e-04, dt: 2760.14, tok/sec: 189950.07\n",
            "step 11, loss: 6.803925037384033, norm: 1.3072, lr: 5.9917e-04, dt: 2761.25, tok/sec: 189873.18\n",
            "step 12, loss: 6.630289077758789, norm: 1.3521, lr: 5.9668e-04, dt: 2768.78, tok/sec: 189357.26\n",
            "step 13, loss: 6.468497276306152, norm: 0.9350, lr: 5.9254e-04, dt: 2768.75, tok/sec: 189358.76\n",
            "step 14, loss: 6.354255199432373, norm: 1.0649, lr: 5.8679e-04, dt: 2770.48, tok/sec: 189241.14\n",
            "step 15, loss: 6.287271976470947, norm: 0.7527, lr: 5.7945e-04, dt: 2767.91, tok/sec: 189416.45\n",
            "step 16, loss: 6.246337413787842, norm: 1.0396, lr: 5.7057e-04, dt: 2775.20, tok/sec: 188918.89\n",
            "step 17, loss: 6.263354301452637, norm: 1.1408, lr: 5.6021e-04, dt: 2773.81, tok/sec: 189013.54\n",
            "step 18, loss: 6.241514682769775, norm: 1.5198, lr: 5.4843e-04, dt: 2774.01, tok/sec: 189000.35\n",
            "step 19, loss: 6.2290263175964355, norm: 1.1042, lr: 5.3531e-04, dt: 2771.36, tok/sec: 189180.49\n",
            "step 20, loss: 6.204236030578613, norm: 0.5783, lr: 5.2092e-04, dt: 2774.69, tok/sec: 188953.86\n",
            "step 21, loss: 6.156164169311523, norm: 0.8413, lr: 5.0535e-04, dt: 2775.42, tok/sec: 188904.24\n",
            "step 22, loss: 6.1336822509765625, norm: 0.6377, lr: 4.8870e-04, dt: 2776.16, tok/sec: 188853.68\n",
            "step 23, loss: 6.087133884429932, norm: 0.5073, lr: 4.7107e-04, dt: 2777.39, tok/sec: 188770.34\n",
            "step 24, loss: 6.069974422454834, norm: 0.7227, lr: 4.5258e-04, dt: 2776.52, tok/sec: 188829.10\n",
            "step 25, loss: 6.054082870483398, norm: 0.3977, lr: 4.3332e-04, dt: 2777.01, tok/sec: 188796.06\n",
            "step 26, loss: 6.029801845550537, norm: 0.5237, lr: 4.1343e-04, dt: 2777.98, tok/sec: 188730.14\n",
            "step 27, loss: 6.041086196899414, norm: 0.6658, lr: 3.9303e-04, dt: 2781.09, tok/sec: 188518.90\n",
            "step 28, loss: 6.007907867431641, norm: 0.4182, lr: 3.7224e-04, dt: 2777.39, tok/sec: 188769.77\n",
            "step 29, loss: 5.994118690490723, norm: 0.3897, lr: 3.5118e-04, dt: 2780.09, tok/sec: 188586.90\n",
            "step 30, loss: 5.986265182495117, norm: 0.3032, lr: 3.3000e-04, dt: 2777.07, tok/sec: 188791.49\n",
            "step 31, loss: 5.962883472442627, norm: 0.4723, lr: 3.0882e-04, dt: 2778.49, tok/sec: 188695.25\n",
            "step 32, loss: 5.965987682342529, norm: 0.4572, lr: 2.8776e-04, dt: 2780.66, tok/sec: 188547.76\n",
            "step 33, loss: 5.931903839111328, norm: 0.2701, lr: 2.6697e-04, dt: 2783.56, tok/sec: 188351.38\n",
            "step 34, loss: 5.920682430267334, norm: 0.2107, lr: 2.4657e-04, dt: 2778.74, tok/sec: 188678.38\n",
            "step 35, loss: 5.918229579925537, norm: 0.2276, lr: 2.2668e-04, dt: 2781.90, tok/sec: 188463.99\n",
            "step 36, loss: 5.902226448059082, norm: 0.2157, lr: 2.0742e-04, dt: 2782.51, tok/sec: 188422.66\n",
            "step 37, loss: 5.913430690765381, norm: 0.2646, lr: 1.8893e-04, dt: 2783.23, tok/sec: 188373.74\n",
            "step 38, loss: 5.8901286125183105, norm: 0.2799, lr: 1.7130e-04, dt: 2781.17, tok/sec: 188513.20\n",
            "step 39, loss: 5.886214256286621, norm: 0.2382, lr: 1.5465e-04, dt: 2781.73, tok/sec: 188475.20\n",
            "step 40, loss: 5.886637210845947, norm: 0.1887, lr: 1.3908e-04, dt: 2785.41, tok/sec: 188226.32\n",
            "step 41, loss: 5.873619556427002, norm: 0.2555, lr: 1.2469e-04, dt: 2783.43, tok/sec: 188360.54\n",
            "step 42, loss: 5.888148784637451, norm: 0.2230, lr: 1.1157e-04, dt: 2787.47, tok/sec: 188087.17\n",
            "step 43, loss: 5.864591598510742, norm: 0.1880, lr: 9.9787e-05, dt: 2786.23, tok/sec: 188171.25\n",
            "step 44, loss: 5.862983703613281, norm: 0.2022, lr: 8.9428e-05, dt: 2785.09, tok/sec: 188248.31\n",
            "step 45, loss: 5.866092205047607, norm: 0.2248, lr: 8.0553e-05, dt: 2790.32, tok/sec: 187895.22\n",
            "step 46, loss: 5.85286808013916, norm: 0.2268, lr: 7.3215e-05, dt: 2786.32, tok/sec: 188164.75\n",
            "step 47, loss: 5.871726989746094, norm: 0.2235, lr: 6.7460e-05, dt: 2785.42, tok/sec: 188225.68\n",
            "step 48, loss: 5.849121570587158, norm: 0.2438, lr: 6.3324e-05, dt: 2784.57, tok/sec: 188283.31\n",
            "step 49, loss: 5.84865665435791, norm: 0.2143, lr: 6.0832e-05, dt: 2790.39, tok/sec: 187890.68\n",
            "tensor(0.1769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "torch.Size([16, 1024, 50304])\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import inspect\n",
        "import os\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    assert config.n_embd % config.n_head == 0\n",
        "    super().__init__()\n",
        "    #key, query, value projections for all heads, but in a batch\n",
        "    self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "    #output projection\n",
        "    self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "    #regularization\n",
        "    self.n_head = config.n_head\n",
        "    self.n_embd = config.n_embd\n",
        "    #not really a bias more of a mask, but following Openai/HF naming\n",
        "    self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1,1,config.block_size, config.block_size))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B, T, C = x.size() # batch, size, sequence legnth\n",
        "    #calculate query , key, values for all heads in batch and move head forward to be the batch\n",
        "    #nh is 'number of heads', hs is 'head size', and C (number of channels) = ns * hs\n",
        "    #eg in GPT-2 (124M), n_head = 12, hs = 654, so nh*hs = 768 channels in the transformer\n",
        "    qkv = self.c_attn(x) #(B,T,3*C)\n",
        "    q,k,v = qkv.split(self.n_embd, dim=2) #(B,T,C), (B,T,C), (B,T,C)\n",
        "    #nh is number of heads, hs is head size\n",
        "    k = k.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    q = q.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    v = v.view(B,T,self.n_head, C//self.n_head).transpose(1,2) #(B,nh,T,hs)\n",
        "    #attention (materializes the large (T,T) matrix for all the queries and keys)\n",
        "\n",
        "    #REGULAR ATTENTION APPROACH\n",
        "    # att = (q @ k.transpose(-2, -1)) * (-1.0 / math.sqrt(k.size(-1)))\n",
        "    # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "    # att = F.softmax(att, dim=-1)\n",
        "    # y = att @ v # (B,nh, T, T) X [B, nh, T, hs] -> (B,nh,T,hs)\n",
        "\n",
        "    #FLASH ATTENTION APPROACH\n",
        "    y = F.scaled_dot_product_attention(q,k,v, is_causal=True)\n",
        "\n",
        "    y = y.transpose(1,2).contiguous().view(B,T,C) # reassemble all head outputs side by side\n",
        "    #output projection\n",
        "    y = self.c_proj(y)\n",
        "    return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "    self.gelu = nn.GELU(approximate='tanh')\n",
        "    self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "    self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.c_fc(x)\n",
        "    x = self.gelu(x)\n",
        "    x = self.c_proj(x)\n",
        "    return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "    self.attn = CausalSelfAttention(config)\n",
        "    self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "    self.mlp = MLP(config)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.attn(self.ln_1(x))\n",
        "    x = x + self.mlp(self.ln_2(x))\n",
        "    return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "  block_size: int = 1024 #max sequence length\n",
        "  vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftoken|>\n",
        "  n_layer: int = 12 #number of layers\n",
        "  n_head: int = 12 #number of heads\n",
        "  n_embd: int = 768 #embedding dimension\n",
        "\n",
        "class GPT(nn.Module):#turns into pytorch module\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.config = config\n",
        "\n",
        "    self.transformer = nn.ModuleDict(dict( #we can query using name of param (identical to GPT2)\n",
        "        wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "        wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "        h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]), #all the different blocks one after the other, #we can query using number of layer (identical to GPT2)\n",
        "        ln_f = nn.LayerNorm(config.n_embd) #additional thing added to transformer arch by openai\n",
        "    ))\n",
        "    self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False) #final linear layer\n",
        "\n",
        "    #weight sharing scheme\n",
        "    #why we do this ?\n",
        "    # 1. this weight is 768*50257 = 40 mil params, (30% of 124M). So makes training more efficient\n",
        "    # 2.  Input embeddings and output embeddings represent the same space: tokens ↔ embeddings.\n",
        "    # 3. Tying ensures consistency: the representation used to encode a word is also used when predicting it.\n",
        "    # 4. Empirically, this improves perplexity\n",
        "    self.transformer.wte.weight = self.lm_head.weight #(vocab_size, n_embd)\n",
        "\n",
        "    #init params\n",
        "    #iteralte all modules here\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  #Prevent vanishing/exploding activations\n",
        "  def _init_weights(self, module):\n",
        "    if isinstance(module, nn.Linear):\n",
        "      std = 0.02\n",
        "      #per-layer scaling trick applied to some linear layers (specifically the projection layers inside attention and MLP blocks)\n",
        "      #It rescales the initialization standard deviation to account for residual connections.\n",
        "      #This way, variance is preserved across depth, so activations don’t explode/vanish.\n",
        "      if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "        std *= (2 * module.NANOGPT_SCALE_INIT) ** -0.5  #for variance handling\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std = std)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "  def configure_optimizers(self, weight_decay, learning_rate, device):\n",
        "    #start with all of the candidate parameters(that require grad)\n",
        "    param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "    param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "    # create optim groups. Any parameters that is 2D+ will be weight decayed, otherwise no.\n",
        "    # ie all weight tensors in matmuls + embeddings decay, all biases and layernorms dont\n",
        "    # notes(16)\n",
        "    decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "    nondecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "    optim_groups = [\n",
        "        {'params': decay_params, 'weight_decay': weight_decay},\n",
        "        {'params': nondecay_params, 'weight_decay': 0.0}\n",
        "    ]\n",
        "    num_decay_params = sum(p.numel() for p in decay_params)\n",
        "    num_non_decay_params = sum(p.numel() for p in nondecay_params)\n",
        "    print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "    print(f\"num non-decayed parameter tensors: {len(nondecay_params)}, with {num_non_decay_params:,} parameters\")\n",
        "    #Create AdamW optimizer and use the fused version if it is available\n",
        "    # notes(17)\n",
        "    fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "    use_fused = fused_available and 'cuda' in device\n",
        "    print(f\"using fused AdamW:  {use_fused}\")\n",
        "    optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "    return optimizer\n",
        "\n",
        "  def forward(self, idx, targets = None):\n",
        "    # idx is of shape (B,T) , token indices, batch dim of B, time dim of upto t, t\n",
        "    B, T = idx.size()\n",
        "    assert T<= self.config.block_size, f\"Cannot forward sequence of length {T},block size is\"\n",
        "    #forward the token and position embeddings\n",
        "    pos = torch.arange(0, T, dtype = torch.long, device = idx.device) #shape(T)\n",
        "    pos_emb = self.transformer.wpe(pos) #POSITION EMBEDDINGS of shape (T,n_embd)\n",
        "    tok_emb = self.transformer.wte(idx) #token embeddings of shape (B,T,n_embd)\n",
        "    x = tok_emb + pos_emb\n",
        "    #forward the blocks of the transformer\n",
        "    for block in self.transformer.h:\n",
        "      x = block(x)\n",
        "\n",
        "    x = self.transformer.ln_f(x)  #(B,T,n_embd)\n",
        "    logits = self.lm_head(x) #(B,T,vocab_size) - essentially give for each batch, gives t+1th probable element\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      # here cross entropy cannot take 3 dim vector, so it is flattening it by first making logits of shape (B*T, vocab_size), and targets to shape (B*T)\n",
        "      loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "    return logits, loss\n",
        "\n",
        "\n",
        "  @classmethod\n",
        "  def from_pretrained(cls, model_type):\n",
        "    '''Loads pretrained GPT2 model weights from huggingface'''\n",
        "    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "    #n_layer, n_head, and n_embd are determined from model_type\n",
        "    config_args = {\n",
        "        'gpt2': dict(n_layer=12, n_head=12, n_embd=768), #124M Params\n",
        "        'gpt2-medium': dict(n_layer=24, n_head=16, n_embd=1024), #350M params\n",
        "        'gpt2-large': dict(n_layer=36, n_head=20, n_embd=1280), #774M params\n",
        "        'gpt2-xl': dict(n_layer=48, n_head=25, n_embd=1600), #1558M params\n",
        "    }[model_type]\n",
        "    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "    # create a from scratch initialized miniGPT model\n",
        "    config = GPTConfig(**config_args)\n",
        "    model = GPT(config)\n",
        "    sd = model.state_dict()\n",
        "    sd_keys = sd.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] #discard this mask/buffer\n",
        "\n",
        "    #init a huggingface trasformer model\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    sd_hf = model_hf.state_dict()\n",
        "\n",
        "    sd_keys_hf = sd_hf.keys()\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] #discard this mask/buff\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "    assert len(sd_keys) == len(sd_keys_hf), f\"mismatched keys: {len(sd_keys)} != {len(sd_keys_hf)}\"\n",
        "    for k in sd_keys:\n",
        "      if any(k.endswith(w) for w in transposed):\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k].t())\n",
        "      else:\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "          sd[k].copy_(sd_hf[k])\n",
        "    return model\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#DATA LOADER\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T, process_rank, num_processes):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "    self.process_rank = process_rank\n",
        "    self.num_processes = num_processes\n",
        "\n",
        "    #load tiny shakespeare dataset\n",
        "    !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "      text = f.read()\n",
        "\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f\"loaded {len(self.tokens)} tokens\")\n",
        "    print(f\"1 epoch = {len(self.tokens) // (B*T)} batches\")\n",
        "\n",
        "    #state\n",
        "    self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "\n",
        "    # Now, Process token sequences and feed them into transformer. Rearrange tokens into idx variable feeding into transformer.\n",
        "    # We dont want single very long one dimensional sequence, we want a batch where each sequence is upto T tokens (T cannot be larger than maximum sequence length).\n",
        "    # We have B indpendent examples of T sequences. So we, need to create a (B,T) tensor which we can feed to the forward out of this 1 dimensional sequences\n",
        "    buf = self.tokens[self.current_position : self.current_position+B*T+1] # B*T + 1 because we need the next token of the last (B*Tth) token as well for training\n",
        "    x = buf[:-1].view(B,T) #we exclude last one because it is the extra target token for (B*Tth) token\n",
        "    y = buf[1:].view(B,T) # we exclude first one because it is not a target for any token\n",
        "\n",
        "    #advance the position in the tensor\n",
        "    self.current_position += B * T * self.num_processes\n",
        "    #if loading the next batch would be out of bounds, reset\n",
        "    if self.current_position + (B*T* self.num_processes+1) > len(self.tokens):\n",
        "      self.current_position = self.B * self.T * self.process_rank\n",
        "\n",
        "    return x, y\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "#TRAINING\n",
        "\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "\n",
        "#notes(19)\n",
        "# set up DDP (distributed data parallel)\n",
        "# torchrun command sets the env variables RANK, LOCAL RANK, and world_size\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 #is this a ddp run?\n",
        "if ddp:\n",
        "  # use if DDP atm demands CUDA, we set the device appropriately according to rank\n",
        "  assert torch.cuda.is_available(), \"for i think we need CUDA for DDP\"\n",
        "  init_process_group(backend='nccl')\n",
        "  #ddp_rank identifies the process globally while ddp_local_rank determines which GPU a process should use locally\n",
        "  ddp_rank = int(os.environ['RANK']) #unique identifier for each process in the entire distributed setup, across all nodes and GPUs\n",
        "  ddp_local_rank = int(os.environ['LOCAL_RANK']) #rank of gpu on single node (Node-local GPU ID (per machine))\n",
        "  ddp_world_size = int(os.environ['WORLD_SIZE']) #total number of processes running\n",
        "  device = f'cuda:{ddp_local_rank}' #indicates which gpu to use\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank == 0 #this process will do logging, checkpointing, etc\n",
        "else:\n",
        "  #vanilla, non-DDP run\n",
        "  ddp_rank = 0\n",
        "  ddp_local_rank = 0\n",
        "  ddp_world_size = 1\n",
        "  master_process = True\n",
        "  # attempt to autodetect device\n",
        "  device = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "  elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "  print(f\"using device: {device}\")\n",
        "\n",
        "#for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(1337)\n",
        "\n",
        "# Gradient accumulation init\n",
        "# refer note(18)\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 16 #micro batch size\n",
        "T = 1024 #sequence length\n",
        "#each forward backward pass processes B * T * ddp_world_size data points\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "if master_process:#only print this a single time, no need ot print it 8 times\n",
        "  print(f\"total desired batch size: {total_batch_size}\")\n",
        "  print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(B=16, T=1024, process_rank=ddp_rank, num_processes=ddp_world_size) #to make it aware of multi process setting. We dont want each process load exact same data. we want them to load different chunks\n",
        "torch.set_float32_matmul_precision('high') #for faster training. Every where there is a multiplcation in our linear layers, pytorch will now run this mult on cores, utilizung tf32 precision\n",
        "\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "# model.eval()\n",
        "# model.to(device)\n",
        "# print('didnt crash yay!!')\n",
        "model = GPT(GPTConfig(vocab_size=50304)) #padding of tokens to improve speed. check notes(11). World size number of these created\n",
        "model.to(device) #all move to their respctive devices\n",
        "\n",
        "#compile for neural nets, like gcc in c. makes stuff faster. reduces python overhead and GPU read/writes\n",
        "#unlike python interpreter that looks at code sequentially, this looks at all the operations ahead and time and then optimizes things based on that.\n",
        "model = torch.compile(model) #world_size numnber of compilations happening in parallel\n",
        "\n",
        "if ddp:\n",
        "  model = DDP(model, device_ids=[ddp_local_rank]) #nothing changed in forward pass. in backward pass, each individual gpu has gradient, what ddp does is calls allreduce, and averages across all the ranks of their gradients and then it will deposit average on every single rank\n",
        "raw_model = model.module if ddp else model # always containes the raw unwrapped model\n",
        "\n",
        "##Learning Rate Scheduler\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "  #1. linear warmup for warmup_iters steps\n",
        "  if it < warmup_steps:\n",
        "    return max_lr * (it+1) / warmup_steps #LR ramping up linearly\n",
        "  #2. if it > lr_decay_iters, return min learning rate. Learning rate stays at min_lr (flat) after training schedule finishes.\n",
        "  if it > max_steps:\n",
        "    return min_lr\n",
        "  #3. in between, use cosine decay down to min learning rate\n",
        "  #LR decays cosine-shaped from max_lr down to min_lr\n",
        "  decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "  assert 0 <= decay_ratio <= 1\n",
        "  coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) #coeff starts at 1 and goes to 0\n",
        "  return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "#optimize!!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay = 0.1, learning_rate=6e-4, device=device)\n",
        "\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "  #refer note(18)\n",
        "  loss_accum = 0.0\n",
        "  for micro_step in range(grad_accum_steps):\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    with torch.autocast(device_type=device, dtype=torch.bfloat16): # TF32 for FP32 matmuls outside autocast. BF16 for eligible ops inside autocast, FP32 for critical ops (loss, gradients, optimizer state)\n",
        "      logits, loss = model(x, y)\n",
        "    loss = loss / grad_accum_steps #average loss across all micro   steps.  #because cross entropy has reduction by mean\n",
        "    loss_accum += loss.detach() #to print the accumulate loss of each micro step in this iteration rather than the loss of the final microstep\n",
        "    if ddp:\n",
        "      model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1) #when microstep is last step, do all reduce and sync gradients across gpus\n",
        "    loss.backward() #deposits gradients, every single microstep, gradients will add up on the gradient tensors on every single call\n",
        "  if ddp:\n",
        "    dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG) #loss accum exists on all ranks, when we call this, it creates the average of numbers and deposits that average on all the ranks\n",
        "  #when this microstep loop is over, and we come out, every single rank will have the average of all gradients that were stored on all ranks, loss_accum is same across all ranks as well\n",
        "\n",
        "\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) #calculate global norm of params, every single grad on all params, we square it, add it all up, and take a big square root and this is the norm of param vector. check note(13)\n",
        "  #determine and set the learning rate for this iteration\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups: #there is a notion of diff param groups that can exist in optimizer. in our case there is just 1, but we have to do this\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize() #cpu might already reach here, but this will wait for gpus to finish the work they were assigned. is only needed for benchmarking. For real training you can drop it (but keep it if you want accurate dt timings).\n",
        "  t1 = time.time()\n",
        "  dt = (t1-t0)*1000 #time difference in miliseconds\n",
        "  tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "  tokens_per_sec = tokens_processed / (t1-t0)\n",
        "  if master_process:\n",
        "    print(f'step {step}, loss: {loss_accum.item()}, norm: {norm:.4f}, lr: {lr:.4e}, dt: {dt:.2f}, tok/sec: {tokens_per_sec:.2f}')\n",
        "\n",
        "if ddp:\n",
        "  destroy_process_group()\n",
        "\n",
        "print(loss)\n",
        "print(logits.shape)\n",
        "#\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "\n",
        "1. In pytorch you can directly do model.to(device), but for tensors, you need to save them in variable x = x.to(device)\n",
        "2. All tensors in pytorch by default are float32 which 19.5 teraflops on A100. If numbers have fewer bits of representation, it is easier to move them around. We have a finite capacity of bits our GPU can store but in addition to that there is a limitation to the speed with which we can access. Many of deep learning workload memory bounds, so most of tensor cores are idle because they are waiting around for data. So even if we get 60% utilization of hardware, we are doing well\n",
        "3. Most of the computation happens in linear layers. Entire transformer is bunch of matrix multiplicaton. Biggest one is the classifier layer at the top that goes from 768 to 50257. Matrix mult becomes faster by lowering precision\n",
        "4. TF32 is 13 bits lesser in mantissa than FP32. Inputs and outputs are fp32, but internally it switches to tf32 and this increases speed by 8 times, and we cannot tell much difference in the results.\n",
        "5. Numbers like 16 and 32 are good for Batches and Time. But something like 17 is bad.\n",
        "6. Lowering precision in model training typically decreases training time because lower-precision data (like FP16 compared to FP32) allows hardware to perform computations faster, requires less memory bandwidth, and enables higher parallelism—so more operations happen per clock cycle. However, we expect slight less accurate results but empirically this is a worth it tradeoff. Because you can train longer to make up for the precision decrease\n",
        "7. HBM is connected with GPU. GPU is where most calc happens, but it also has some memory. Most of memory is in high bandwidth memory(HBM). These are two separate chips. HBM is off chip. On GPU, there are large number of streaming processor all of which are SM, and this is where lot of the calculations happen. Single SM has 4 quadrants, each has a tensor core, and different subunits where calcs happen. On GPU chips there is L2 cache, but then on the SM in GPU, there is l1 cache,and registers. The way the memory is stored on GPU is quite different from HBM. So, there is meomery inside GUP but it is not a lot of memory.\n",
        "8. Now, even if main memory of computer is very large, it is very inefficient as GPU would have to go through CPU to reach disk, and this is very time intensive. Then GPUs have HBM which are large in memory, but are also expensive to access. Then, on the GPU chips itself everything is very fast, but there is very few memory on it(in MBs as opposed to gbS), but it is lightning fast. So basically whenver, we have these kernels, we take these inputs which live on HBM, we start streaming data to gpu chip, we do computations, and then send it back to HBM. If we dont use torch.compile, we we are doing this HBM->GPU->HBM transfers many more times. But when we use it, since it already know stuff ahead in time, it optimizes data transfer and does all the computations together. So operation fusion, allows to keep chunk of data on chip, do lots of computation, and then do a single transfer back to HBM.\n",
        "9. Flash attention: Fusing matmul, dropout, softmax, mask, and matmul to a fused kernel of flash attention. It is a kernel fusing algorithm torch.compile cannot find, and the reason is algorithmic rewrite of how attention is implemented. Flashattention does more flops(arithmetic ops) than regular attention. But it is significantly faster(7.6x) because very mindful of the memory heirarchy described above. Very mindful about what is in HBM, what is in shared memory. Very careful of how it orchastrates the computation, such that we have a fewer reads and writes with HBM. So even though we are doing more flops, the expensive part is the load and store. The NXN matrix (att in our code), never gets materialized at any point in HBM, and never gets read or written to HBM. For each head,\n",
        "10. It is good to deal with powers of two because that is how cuda works\n",
        "11. When we pass in custom vocab_size=50304, which is > 50257, (to make a an ugly number into a nicer number power of 2 to increase speed), wte becomes larger, but these newer tokens rows/vectors are never used because GPT2 only has 50257 tokens. We will never index into these rows, so wasting a little bit of space. Now, it is share with classifier at the end,  so we are predicting probabilities for tokens that will never be present in the training set and so therefore the network has to learn that these probabilities have to be driven to 0. And so the logits that the network produces have to drive those dimensions of the output to negative infinity. But this is no different to tokens not present in our dataset, ie Shakespeare dataset only uses 1000 of the 50000+ tokens. So functionally nothing breaks, we just use extra memory. It is running faster because many kernels use block tiles which are powers of 2, so calcs done in chunks of 64. When desired calc does not neatly fit into those block tiles, there are all kind of boundary kernels that can kick in to do the last part. In a lot of kernels, they will truncate up your input and will do the nice part first and then they have a whole second phase where they come back to anything that remains and process it, but this could be very inefficient. So instead pad the input, and make it fit nicely.\n",
        "12. According to Andrej, gpt 2 has open weights, but paper does not have much detail for training, for gpt 3, paper has a lot of info, but weights not released. Roughly speaking they are very similar architecturally  , apart from some hyperparameters,context length size, training time, more data\n",
        "13. We make sure length is not more than 1.0. People like to use this gradient norm clipping, because sometimes you can get unlucky with optimization because of bad data batch or something like that. If you get very unlucky in batch you might get really high loss and really high loss could lead to really high gradient and this could basically shock your model and the optimization . So this prevents model from getting too big of shocks in terms of gradient magnitudes. Bit of a hacky solution, patch on top of bigger issues. You can visualize this norm, and if norm of gradient is horizontal, it is well behaved and fine, if it is climbing, things are bad, sometimes you can get spike which is also bad due to training instability. Norm very high in begining as learning a lot of new stuff.\n",
        "14. For context, I trained the model on A100 GPU\n",
        "15. Cosine learning rate that we used here has been popularize by gpt 2 and 3 papers, but there are other learning rate schedules and this is an active area of research.\n",
        "16. We weight decay weights involved in matrix multiplications, embeddings, etc, but not those which are biases, or layer norms, etc. We decay weights, because it is like regularization. when you are pulling down weights all the weights, you are forcing the optimization to use more of the weights and you are not allowing any of the weights to be way too large. You are forcing the network to distribute the work across more channels\n",
        "17. Fused AdamW - Not available by default, need to turn it on when running on cuda. What it does is, instead of running in a for loop over all the paramter tensors and updating them, that would launch a lot of kernels. Fused just means all those kernels are fused into a single kernel, you get rid of a lot of overhead, and you single time on all the parameters call a kernel that updates them. So it is basically a kernel fusion for AdamW update instead of updating all the tensors.\n",
        "18. In different variants of GPT3, the bigger networks are trained with lower lr. The batch size larger for larger networks. For the 125M Params version of GPT3, its batch size is 0.5M. But we cannot do this if we have a small GPU. To simulate this, we can do gradient accumulation, it allows to simulate in a serial way any arbitrary batch size we want to set. So to set it to 0.5M, we just have to run longer, and we have to process multiple sequence and basically add up all the gradients from them to simulate a batch size of 0.5M\n",
        "19. Distributed Data Parallel: We have 8 GPU, so we launch 8 processes and each process will be assigned to a GPU. For each process, the training loop and everything we worked on looks same. But now there are 8 of them and they will all process slightly diff parts of the data. We add one more part where once they all calculate their own gradients there is one more part where we do average of those gradients and that is how they collaborate. To use all 8, we don't just run the cell, we run with torchrun and when it runs python script it run all gpu on parallel and creates environment variables where each can look up and see which process it is. Only the ddp_rank of GPU's will be diff otherwise everything else same. Analogous to 8 python interpreters running down code and only diff betn them is their ddp_rank/\n",
        "20. GPT2 use webtext dataset which was not released. Open webtext was created to replicate it. 50% of tokens common crawl, then webtext2, books, wikipedia."
      ],
      "metadata": {
        "id": "iiLlZe0lXFj_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5S6tGw0rYOOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30"
      ],
      "metadata": {
        "id": "5CxvvkgNwDbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78tIQpndwDYz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}